{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review for Mod 3 # Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: written, will cover: calculus, cost function, gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a cost function? Talk about the differences and residual sum of squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cost function is a plot of different residual sum of squares(RSS) values for m. The curve demonstrates the lowest RSS. The curve helps to optimise towards a minimum value, known as the gradient descent. Our cost curve will take m_values as the input for x and rss errors as the output values. The cost curve after plotting all of these points show a changing output of RSS based on a changing output of different regression lines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to find the lowest point we begin to try to guess and check. This can create overshooting. Overshooting takes place when the steps taken to find the lowest RSS are missed or over represented. A stepwise function helps to break this down in order to more accuractly find the lowest value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *RSS and MSE as a cost function (talk about how you can use different ones, but these are the most common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does gradient descent work in order to find the best point(min) slope? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is a technique that finds the lowest minimum value. It works by descending along the cost curve. As the variable changes, it's crucial to stop when the value of the RSS no longer decreases. \n",
    "Gradient descent takes the shortest path to descend toward the minimum. It finds the direction of the greatest decrease, and simply reverses the direction of of the partial derivative and moves in change of f'-y and change in f'-x. \n",
    "gradient descent is this whole line, repeated over again which produces this process. (new weight)= previous weight - learning rate*m, this is our new weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the best slope in the process so we know how to change the weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the learning rate? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate basically multiplying the slope by a fraction. The learning rate helps to avoid the risk of overshooting the minimum value. The learning rate should move in the opposite direction of the slope. When the slope of a cost curve points downwards, we want to move higher. If it moves upwards, we want our learning rate to move lower. \n",
    "we decide the learning rate. if you notice that you're overshooting even after gradient descent then we know our learning rate is too high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: written and some computation, will cover: confusion matrix, precision, recall, F1, and all metrics, ROC curve, class imbalance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a confusion matrix? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a performance measure that tells you 4 important things which is represented through a grid. \n",
    "    1. True Positives, which are the number of observations where the model predicted shows that what you're testing for is working correctly in favor. \n",
    "    2. True Negatives, the number of observations where the model predicted shows that what we are testing for is working correctly to show there are things that correctly fall negative. \n",
    "    3. False positives(type 1 error), misdiagnosing \n",
    "    4. False negative(type 2 error) , essentially telling someone another misdaignosis. \n",
    "A confusion matrix is extremely important because it helps to obtain other crucial metrics, such as: precision, recall, an F1 score, accuracy and most importantly a ROC-Curve and AUC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you obtain precision, recall and an F1 score, along with all the other metrics (TPs, TNs, FPs, FNs)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision is a positive predictive value, it measures how precise the predictions are. \n",
    "    Precision is computed by: # of true positives/(# of predicted positives) ~ TP/(TP+FP)\n",
    "    \n",
    "#### Recall is also known as the true positive rate, sensitivity, p of detection, or power. Recall indicates what percentage of the classes we're interested in were actually captured by the model. \n",
    "    Recal is computed by: True positives/ (true positives + false negatives) ~ TP/(TP+FN)\n",
    "Precision and recall have an inverse relationship. As recall goes up, precision will go down and vice versa. \n",
    "\n",
    "#### F1 is represents harmonic mean of precision and recall. This means that the F1 score cannot be high without both precision and recall also being high. When youre F1 model is doing well, you know your model is doing well altgother. An F1 score penalises models heavily if it skews too hard towards either precision or recall. For this reason, an F1 score is generally used the most for describing the performance for a model. \n",
    "    F1 is computed by: 2*precision*recall/(precision+recall) \n",
    "    \n",
    "#### Accuracy provides a solid holisitc view of the overall performance model. It is useful because it allows us to measure the total number of predictions of a model gets right, including true positives(TPs) and true negatives(TNs). \n",
    "    it is computed by: (true positives + true negatives)/(true positives+true negatives+false positives+false negatives) ~ (TN + TP)/(TN+TP+FP+FN)\n",
    "    \n",
    "##### TP is true positive\n",
    "    it is computed by: True Positive/(True positive+False Negative) ~ TP/(TP+FN)\n",
    "    \n",
    "##### TN is true negative \n",
    "    it is computed by: True Negative/(True Negative+False Positive) ~ TN/(TN+FP)\n",
    "    \n",
    "##### FP is false positive\n",
    "    it is computed by: False Positive/(False Positive+True Negative)\n",
    "    \n",
    "##### FN is false negative\n",
    "    it is computed by: \n",
    "    \n",
    "For (precision, recall, f1) can use sklearn.metrics for this\n",
    "    \n",
    "from sklearn.metrics import confusion_matrix(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##INSERT HOW TO OBTAIN THESE FROM LECTURE YESTERDAY\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a ROC curve? What does it represent? Which ROC curve do you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC stands for reciever operator characteristic. It illustrates the true positive rate against the false positive rate of our classifier. The ROC plots all of the thresholds from multiple confusuin matrices to give us a dotted line that makes up an overall 'curve'. ROC curves makes it easier to identify the best threshold for making a decision.\n",
    "We ideally want to have a ROC curve that is close to the upper left corner. A ROC curve gives us a graph of the tradeoff between this false positive rate and the true positive rate. \n",
    "We ideally want to use the roc curve with the best accuracy score between the train and validation set, but an ROC and confusion matrix can be plotted using multiple different models, we could use a logistic regression model, and/or a random forest model. After plotting these ROC curves you pick the curve that represents the higher AUC (area under the curve) and this will be the best representation. We know that there may be discrepancies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is class imbalance? How do you detect it? What are the consequences? How do you fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class imbalance is when the datapoints for each category are not the same. You detect class imbalance through value_counts. This helps you to see where there is an imbalance between the classes. This can cause difficulties for the learning algorithm. \n",
    "You can fix this by passing 'balanced' within the classes so that the weights are inversely proportional, but this is not a preferred method. \n",
    "You can use a method called SMOTE to help fix this. SMOTE stands for synthetic minority oversampling. It is an algorithm that generates new sample data. The consequence of this is that you're teaching your machine learning algorithm to become biased in its generalisation. This means that as the algorithm keeps running it will stick to modelling only in the way of the class imbalance. \n",
    "You can also use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: written but mostly computation, will cover: decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a decision tree? How does it work? How does it split? What is the most pure outcome?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a graphical representation used to parition sample spaces as efficiently as possible into sets with similar data points until you get to a homogenous set. It can reasonably predict the value for new data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree can either be a regressor or a classifier. A decision tree regressor predicts continuous values like the price of a house. A decision tree classifier works on classifying categorical data, it helps to identify which class certain data falls into. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision to split at each node is called 'purity'. A node is 100% impure when it is split evenly, 50/50 and 100% pure when it belongs to a single class. \n",
    "Information gain is used to decide on what feature to split at each step in the tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree usually takes in criteria otherwise it will not stop until it is overfit and exhausted itself to the point of perfection. CART which stands for classification and regression trees use the gini index as a metric. Entropy is also another metric that calculates information gain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate some metrics from your decision tree: MSE, MAE, R**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import california_housing\n",
    "data = california_housing.fetch_california_housing()\n",
    "X = pd.DataFrame(data['data'],columns=data['feature_names'])\n",
    "y = pd.Series(data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.00001    965\n",
       "1.37500    122\n",
       "1.62500    117\n",
       "1.12500    103\n",
       "1.87500     93\n",
       "          ... \n",
       "3.85000      1\n",
       "3.93600      1\n",
       "4.12300      1\n",
       "4.33000      1\n",
       "0.47900      1\n",
       "Length: 3842, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "X_train_v, X_val, y_train_v, y_val = train_test_split(X_train, y_train, test_size=.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model = DecisionTreeRegressor(random_state=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      presort=False, random_state=7, splitter='best')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 5.361757105537185e-14\n",
      "MAE: 1.072351436382474e-08\n",
      "r squared: 0.9999999999999599\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE:\", mean_squared_error(y_train, preds))\n",
    "print(\"MAE:\", mean_absolute_error(y_train, preds))\n",
    "print(\"r squared:\", r2_score(y_train, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare your predictions with actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: computational, ensemble methods (random forest tree, type of ensemble methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to instantiate stratified k fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune it using sklearn and gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Supported target types are: ('binary', 'multiclass'). Got 'continuous' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-1eb7fc00de8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'min_samples_leaf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopt_model\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mopt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mopt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    333\u001b[0m                 .format(self.n_splits, n_samples))\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_test_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mtrain_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mtest_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_test_masks\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iter_test_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0mtest_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtest_folds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_make_test_folds\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    639\u001b[0m             raise ValueError(\n\u001b[1;32m    640\u001b[0m                 'Supported target types are: {}. Got {!r} instead.'.format(\n\u001b[0;32m--> 641\u001b[0;31m                     allowed_target_types, type_of_target_y))\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Supported target types are: ('binary', 'multiclass'). Got 'continuous' instead."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'max_depth': range(1,8+1), 'min_samples_leaf':[5,10,15]}\n",
    "opt_model =GridSearchCV(model, param_grid, cv=skf, scoring='roc_auc')\n",
    "opt_model.fit(X_train, y_train)\n",
    "best_model = opt_model.best_estimator_\n",
    "opt_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-a8ee00a9aa44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0mcorresponds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \"\"\"\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimators_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "probas = model.predict_proba(X_train_v)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probas[:,1])\n",
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42105263, 0.42105263, 0.14885496, 0.92982456, 0.14885496,\n",
       "       0.14885496, 0.42105263, 0.42105263, 0.14885496, 0.42105263,\n",
       "       0.42105263, 0.92982456, 0.14885496, 0.14885496, 0.14885496,\n",
       "       0.68965517, 0.42105263, 0.92982456, 0.42105263, 0.42105263,\n",
       "       0.14885496, 0.92982456, 0.92982456, 0.42105263, 0.42105263,\n",
       "       0.68965517, 0.42105263, 0.14885496, 0.42105263, 0.42105263,\n",
       "       0.42105263, 0.42105263, 0.42105263, 0.14885496, 0.42105263,\n",
       "       0.42105263, 0.14885496, 0.14885496, 0.14885496, 0.42105263,\n",
       "       0.42105263, 0.42105263, 0.42105263, 0.14885496, 0.42105263,\n",
       "       0.14885496, 0.14885496, 0.42105263, 0.42105263, 0.14885496,\n",
       "       0.68965517, 0.42105263, 0.42105263, 0.14885496, 0.14885496,\n",
       "       0.92982456, 0.14885496, 0.14885496, 0.42105263, 0.14885496,\n",
       "       0.92982456, 0.14885496, 0.92982456, 0.14885496, 0.14885496,\n",
       "       0.14885496, 0.14885496, 0.14885496, 0.42105263, 0.42105263,\n",
       "       0.14885496, 0.14885496, 0.68965517, 0.42105263, 0.42105263,\n",
       "       0.42105263, 0.14885496, 0.14885496, 0.42105263, 0.14885496,\n",
       "       0.92982456, 0.14885496, 0.14885496, 0.42105263, 0.42105263,\n",
       "       0.42105263, 0.42105263, 0.92982456, 0.14885496, 0.42105263,\n",
       "       0.42105263, 0.42105263, 0.42105263, 0.42105263, 0.42105263,\n",
       "       0.14885496, 0.92982456, 0.42105263, 0.14885496, 0.42105263,\n",
       "       0.42105263, 0.42105263, 0.42105263, 0.92982456, 0.42105263,\n",
       "       0.14885496, 0.14885496, 0.42105263, 0.14885496, 0.42105263,\n",
       "       0.14885496, 0.14885496, 0.42105263, 0.42105263, 0.42105263,\n",
       "       0.42105263, 0.42105263, 0.68965517, 0.14885496, 0.92982456,\n",
       "       0.14885496, 0.14885496, 0.42105263, 0.14885496, 0.42105263,\n",
       "       0.42105263, 0.14885496, 0.92982456, 0.14885496, 0.42105263,\n",
       "       0.42105263, 0.42105263, 0.14885496, 0.14885496, 0.42105263,\n",
       "       0.92982456, 0.14885496, 0.14885496, 0.14885496, 0.14885496,\n",
       "       0.14885496, 0.68965517, 0.92982456, 0.42105263, 0.42105263,\n",
       "       0.14885496, 0.68965517, 0.14885496, 0.14885496, 0.42105263,\n",
       "       0.42105263, 0.42105263, 0.14885496, 0.14885496, 0.42105263,\n",
       "       0.14885496, 0.14885496, 0.14885496, 0.92982456, 0.42105263,\n",
       "       0.14885496, 0.14885496, 0.14885496, 0.14885496, 0.42105263,\n",
       "       0.14885496, 0.14885496, 0.14885496, 0.14885496, 0.14885496,\n",
       "       0.14885496, 0.14885496, 0.42105263, 0.42105263, 0.42105263,\n",
       "       0.42105263, 0.14885496, 0.14885496, 0.14885496, 0.14885496,\n",
       "       0.42105263, 0.14885496, 0.14885496, 0.42105263, 0.14885496,\n",
       "       0.14885496, 0.14885496, 0.42105263, 0.14885496, 0.14885496,\n",
       "       0.14885496, 0.42105263, 0.14885496, 0.92982456, 0.42105263,\n",
       "       0.42105263, 0.42105263, 0.42105263, 0.92982456, 0.14885496,\n",
       "       0.68965517, 0.14885496, 0.14885496, 0.42105263, 0.42105263,\n",
       "       0.42105263, 0.14885496, 0.14885496, 0.42105263, 0.42105263,\n",
       "       0.42105263, 0.14885496, 0.42105263, 0.42105263, 0.42105263,\n",
       "       0.42105263, 0.42105263, 0.14885496, 0.14885496, 0.14885496,\n",
       "       0.42105263, 0.14885496, 0.14885496, 0.92982456, 0.42105263,\n",
       "       0.14885496, 0.42105263, 0.14885496, 0.14885496, 0.42105263,\n",
       "       0.42105263, 0.14885496, 0.14885496, 0.42105263, 0.42105263,\n",
       "       0.42105263, 0.14885496, 0.14885496, 0.14885496, 0.14885496,\n",
       "       0.42105263, 0.14885496, 0.92982456, 0.14885496, 0.42105263,\n",
       "       0.14885496, 0.42105263, 0.42105263, 0.14885496, 0.14885496,\n",
       "       0.42105263, 0.14885496, 0.14885496, 0.42105263, 0.14885496,\n",
       "       0.14885496, 0.42105263, 0.14885496, 0.92982456, 0.14885496,\n",
       "       0.92982456, 0.14885496, 0.68965517, 0.92982456, 0.42105263,\n",
       "       0.42105263, 0.14885496, 0.14885496, 0.42105263, 0.42105263,\n",
       "       0.14885496, 0.42105263, 0.14885496, 0.42105263, 0.68965517,\n",
       "       0.42105263, 0.42105263, 0.14885496, 0.42105263, 0.42105263,\n",
       "       0.14885496, 0.42105263, 0.14885496, 0.42105263, 0.14885496,\n",
       "       0.42105263, 0.92982456, 0.14885496, 0.42105263, 0.92982456,\n",
       "       0.14885496, 0.14885496, 0.14885496, 0.14885496, 0.14885496,\n",
       "       0.68965517, 0.14885496, 0.42105263, 0.14885496, 0.14885496,\n",
       "       0.14885496, 0.42105263, 0.42105263, 0.92982456, 0.42105263,\n",
       "       0.14885496, 0.42105263, 0.42105263, 0.42105263, 0.42105263,\n",
       "       0.14885496, 0.42105263, 0.14885496, 0.14885496, 0.14885496,\n",
       "       0.14885496, 0.42105263, 0.14885496, 0.42105263, 0.14885496,\n",
       "       0.14885496, 0.14885496, 0.68965517, 0.14885496, 0.42105263,\n",
       "       0.14885496, 0.14885496, 0.14885496, 0.42105263, 0.14885496,\n",
       "       0.42105263, 0.92982456, 0.68965517, 0.92982456, 0.42105263,\n",
       "       0.42105263, 0.14885496, 0.14885496, 0.42105263, 0.14885496,\n",
       "       0.68965517, 0.14885496, 0.42105263, 0.42105263, 0.14885496,\n",
       "       0.14885496, 0.42105263, 0.14885496, 0.14885496, 0.42105263,\n",
       "       0.14885496, 0.42105263, 0.42105263, 0.42105263, 0.14885496,\n",
       "       0.14885496, 0.42105263, 0.42105263, 0.14885496, 0.14885496,\n",
       "       0.92982456, 0.92982456, 0.42105263, 0.42105263, 0.68965517,\n",
       "       0.14885496, 0.42105263, 0.14885496, 0.92982456, 0.14885496,\n",
       "       0.42105263, 0.92982456, 0.42105263, 0.42105263, 0.14885496,\n",
       "       0.14885496, 0.42105263, 0.14885496, 0.92982456, 0.42105263,\n",
       "       0.14885496, 0.42105263, 0.14885496, 0.14885496, 0.42105263,\n",
       "       0.14885496, 0.14885496, 0.14885496, 0.42105263, 0.42105263,\n",
       "       0.14885496, 0.14885496, 0.68965517, 0.92982456, 0.42105263,\n",
       "       0.14885496, 0.14885496, 0.42105263, 0.92982456, 0.14885496,\n",
       "       0.42105263, 0.14885496, 0.68965517, 0.14885496, 0.14885496,\n",
       "       0.14885496, 0.42105263, 0.14885496, 0.42105263, 0.14885496,\n",
       "       0.68965517, 0.92982456, 0.14885496, 0.42105263, 0.14885496,\n",
       "       0.14885496, 0.14885496, 0.14885496, 0.68965517, 0.68965517,\n",
       "       0.42105263, 0.14885496, 0.42105263, 0.14885496, 0.42105263,\n",
       "       0.42105263, 0.14885496, 0.14885496, 0.14885496, 0.14885496,\n",
       "       0.14885496, 0.92982456, 0.14885496, 0.14885496, 0.14885496,\n",
       "       0.92982456, 0.68965517, 0.42105263, 0.42105263, 0.42105263,\n",
       "       0.42105263, 0.42105263, 0.14885496, 0.42105263, 0.92982456,\n",
       "       0.14885496, 0.14885496, 0.14885496, 0.42105263, 0.68965517,\n",
       "       0.42105263, 0.92982456, 0.14885496, 0.42105263, 0.14885496,\n",
       "       0.14885496, 0.14885496, 0.14885496, 0.42105263, 0.42105263,\n",
       "       0.42105263, 0.42105263, 0.42105263, 0.42105263, 0.92982456,\n",
       "       0.14885496, 0.92982456, 0.42105263, 0.14885496, 0.14885496,\n",
       "       0.14885496, 0.92982456, 0.92982456, 0.42105263, 0.42105263,\n",
       "       0.14885496, 0.14885496, 0.42105263, 0.14885496, 0.42105263,\n",
       "       0.14885496, 0.14885496, 0.14885496, 0.42105263, 0.42105263,\n",
       "       0.42105263, 0.14885496, 0.14885496, 0.42105263, 0.68965517,\n",
       "       0.14885496, 0.42105263, 0.42105263, 0.14885496, 0.42105263,\n",
       "       0.42105263, 0.42105263, 0.92982456, 0.14885496, 0.14885496,\n",
       "       0.92982456, 0.14885496, 0.68965517, 0.92982456, 0.14885496,\n",
       "       0.14885496, 0.92982456, 0.14885496, 0.14885496, 0.14885496,\n",
       "       0.92982456, 0.14885496, 0.42105263, 0.92982456, 0.14885496,\n",
       "       0.14885496, 0.92982456, 0.14885496, 0.42105263, 0.42105263,\n",
       "       0.14885496, 0.14885496, 0.14885496, 0.42105263, 0.14885496,\n",
       "       0.42105263, 0.14885496, 0.42105263, 0.42105263, 0.92982456,\n",
       "       0.42105263, 0.42105263, 0.14885496, 0.68965517, 0.14885496,\n",
       "       0.42105263, 0.68965517, 0.68965517, 0.92982456, 0.42105263,\n",
       "       0.14885496, 0.14885496, 0.14885496, 0.14885496, 0.42105263,\n",
       "       0.42105263, 0.14885496, 0.14885496, 0.42105263, 0.42105263,\n",
       "       0.14885496, 0.42105263, 0.42105263, 0.42105263, 0.42105263,\n",
       "       0.92982456, 0.14885496, 0.68965517, 0.14885496, 0.14885496,\n",
       "       0.14885496, 0.14885496, 0.42105263, 0.42105263, 0.42105263,\n",
       "       0.68965517, 0.14885496, 0.14885496, 0.42105263, 0.42105263,\n",
       "       0.92982456, 0.14885496, 0.42105263, 0.14885496, 0.42105263,\n",
       "       0.92982456])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is the best method? How do you know?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
